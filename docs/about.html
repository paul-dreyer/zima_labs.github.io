
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <link rel="preconnect" href="https://fonts.gstatic.com"/>
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&amp;display=swap" rel="stylesheet"/>
    <link href="https://api.fontshare.com/v2/css?f[]=clash-grotesk@400,300,500&amp;display=swap" rel="stylesheet"/>
    <link rel="stylesheet" href="css/tailwind/tailwind.min.css"/>
    <link rel="icon" type="image/png" sizes="32x32" href="fav-icon.png"/>
    <script src="https://cdn.jsdelivr.net/npm/alpinejs@3.13.3/dist/cdn.min.js" defer="defer"></script>
  </head>
  <body class="antialiased bg-body text-body font-body">
    <div>
      <section class="relative overflow-hidden z-[9999]" x-data="{ mobileNavOpen: false }">
        <div class="container px-4 mx-auto">
          <div class="flex items-center justify-between pt-6 -m-2">
            <div class="w-auto p-2">
              <div class="flex flex-wrap items-center">
                <div class="w-auto"><a class="relative z-10 inline-block" href="index.html"><img src="images/logo-transparent.png" style="max-height: 60px;" alt=""/></a></div>
              </div>
            </div>
            <div class="w-auto p-2">
              <div class="flex flex-wrap items-center">
                <div class="w-auto hidden lg:block">
                  <ul class="flex items-center mr-12">
                    <li class="mr-12 text-white font-medium hover:text-green-400 tracking-tighter"><a href="index.html">Home</a></li>
                    <li class="mr-12 text-white font-medium hover:text-green-400 tracking-tighter"><a href="about.html">About</a></li>
                    <li class="mr-12 text-white font-medium hover:text-green-400 tracking-tighter"><a href="gallery.html">Data Gallery</a></li>
                    <li class="text-white font-medium hover:text-green-400 tracking-tighter"><a href="documentation.html">Documentation</a></li>
                  </ul>
                </div>
                <div class="w-auto hidden lg:block">
                  <div class="inline-block"><a class="inline-block px-8 py-4 text-black hover:text-white tracking-tighter bg-green-400 hover:bg-green-400 border-2 border-white focus:border-green-400 focus:border-opacity-40 focus:ring-4 focus:ring-green-400 focus:ring-opacity-40 rounded-full" href="contact.html">Contact</a></div>
                </div>
                <div class="w-auto lg:hidden">
                  <button class="relative z-10 inline-block" x-on:click="mobileNavOpen = !mobileNavOpen">
                    <svg class="text-green-500" width="51" height="51" viewbox="0 0 56 56" fill="none" xmlns="http://www.w3.org/2000/svg">
                      <rect width="56" height="56" rx="28" fill="currentColor"></rect>
                      <path d="M37 32H19M37 24H19" stroke="black" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path>
                    </svg>
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="hidden fixed top-0 left-0 bottom-0 w-4/5 max-w-xs z-50" :class="{'block': mobileNavOpen, 'hidden': !mobileNavOpen}">
          <div class="fixed inset-0 bg-black opacity-60" x-on:click="mobileNavOpen = !mobileNavOpen"></div>
          <nav class="fixed top-0 left-0 w-4/5 max-w-xs h-screen bg-black z-50 px-9 pt-8">
            <div class="flex flex-wrap justify-between h-screen">
              <div class="w-full">
                <div class="flex items-center justify-between -m-2">
                  <div class="w-auto p-2"><a class="inline-block" href="#"><img src="images/logo-transparent.png" style="max-height: 150px;" alt=""/></a></div>
                  <div class="w-auto p-2">
                    <button class="inline-block text-white" x-on:click="mobileNavOpen = !mobileNavOpen">
                      <svg width="24" height="24" viewbox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M6 18L18 6M6 6L18 18" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
                      </svg>
                    </button>
                  </div>
                </div>
              </div>
              <div class="flex flex-col justify-center py-16 w-full">
                <ul>
                  <li class="mb-8 text-white font-medium hover:text-opacity-90 tracking-tighter"><a href="index.html">Home</a></li>
                  <li class="mb-8 text-white font-medium hover:text-opacity-90 tracking-tighter"><a href="about.html">About</a></li>
                  <li class="mb-8 text-white font-medium hover:text-opacity-90 tracking-tighter"><a href="gallery.html">Data Gallery</a></li>
                  <li class="text-white font-medium hover:text-opacity-90 tracking-tighter"><a href="documentation.html">Documentation</a></li>
                </ul>
              </div>
              <div class="pb-8"><a class="inline-block px-8 py-4 text-center text-black hover:text-white tracking-tighter bg-green-400 hover:bg-green-400 border-2 border-white focus:border-green-400 focus:border-opacity-40 focus:ring-4 focus:ring-green-400 focus:ring-opacity-40 rounded-full" href="contact.html">Contact</a></div>
            </div>
          </nav>
        </div>
      </section>
      <section class="py-20 overflow-hidden">
        <div class="container px-4 mb-20 mx-auto">
          <div class="md:max-w-xl text-center mx-auto mb-10">
            <h2 class="font-heading text-7xl text-white tracking-tighter-xl">About Synthetic Data</h2>
          </div>
          <div class="max-w-5xl mx-auto">
            <div class="flex flex-wrap items-center justify-center mx-auto">
              <div class="w-full md:w-3/4 px-4 lg:px-12">
                <h2 class="font-heading text-3xl text-green-300 tracking-tighter-xl mb-2">Curriculum Learning</h2>
                <p class="mb-4 text-gray-300 tracking-wider">
                   In robotics, control policies are typically trained on simple 
                  tasks first — like balancing — before progressing to more 
                  complex challenges such as walking or stair climbing. This 
                  approach, known as curriculum learning, helps models learn 
                  more effectively. Yet in computer vision, it's rarely applied,
                  largely because it's hard to define how “difficult” an image 
                  is for a model to interpret.
                </p>
                <p class="mb-10 text-gray-300 tracking-wider">
                   Our datasets change that. Every image-label pair includes rich
                  metadata that captures scene complexity — like object 
                  occlusion, position and  orientation, lighting conditions, etc.
                  — making it easy to structure a learning curriculum. Want to 
                  begin with clear, unobstructed views and gradually introduce 
                  clutter and occlusion? Done. Our synthetic data makes 
                  curriculum learning in vision not only possible — but practical.
                </p>
              </div>
            </div>
          </div>
          <div class="max-w-5xl mx-auto">
            <div class="flex flex-wrap items-center justify-center mx-auto">
              <div class="w-full md:w-3/4 px-4 lg:px-12">
                <h2 class="font-heading text-3xl text-green-300 tracking-tighter-xl mb-2">Domain Randomization</h2>
                <p class="mb-4 text-gray-300 tracking-wider">
                   To build models that generalize to the real world, it's 
                  critical to expose them to wide variations during training — 
                  a practice known as domain randomization. With synthetic data,
                  you have full control: vary lighting, textures, object 
                  placement, camera angles, even add fog, dust, or simulated 
                  wear.
                </p>
                <p class="mb-10 text-gray-300 tracking-wider">
                   Want to train for warehouse conditions with poor lighting and 
                  dust buildup? Simulate it. Need your model to handle objects 
                  flipped, stacked, or partially occluded? No problem. Our 
                  datasets retain all randomization parameters, so you can 
                  filter, analyze, or subset based on them before and after training.
                </p>
              </div>
            </div>
          </div>
          <div class="max-w-5xl mx-auto">
            <div class="flex flex-wrap items-center justify-center mx-auto">
              <div class="w-full md:w-3/4 px-4 lg:px-12">
                <h2 class="font-heading text-3xl text-green-300 tracking-tighter-xl mb-2">Related Papers</h2>
              </div>
              <div class="w-full md:w-3/4 px-4 lg:px-12" x-data="{ expanded: false }">
                <h2 class="font-heading text-2xl text-white tracking-tighter-xl">Synthetic Image Data for Deep Learning</h2>
                <p class="transition-all duration-300" :class="expanded ? 'text-gray-300 tracking-wider' : 'text-gray-300 line-clamp-1'">
                  Abstract—Realistic synthetic image data rendered from 3D
                  models can be used to augment image sets and train image
                  classification semantic segmentation models. In this work, we
                  explore how high quality physically-based rendering and domain
                  randomization can efficiently create a large synthetic dataset
                  based on production 3D CAD models of a real vehicle. We use this
                  dataset to quantify the effectiveness of synthetic augmentation
                  using U-net and Double-U-net models. We found that, for
                  this domain, synthetic images were an effective technique for
                  augmenting limited sets of real training data. We observed that
                  models trained on purely synthetic images had a very low mean
                  prediction IoU on real validation images. We also observed that
                  adding even very small amounts of real images to a synthetic
                  dataset greatly improved accuracy, and that models trained on
                  datasets augmented with synthetic images were more accurate
                  than those trained on real images alone.
                  Finally, we found that in use cases that benefit from incremen-
                  tal training or model specialization, pretraining a base model on
                  synthetic images provided a sizeable reduction in the training cost
                  of transfer learning, allowing up to 90% of the model training
                  to be front-loaded.
                </p>
                <button class="text-green-400 text-sm font-medium flex items-center space-x-1 hover:underline focus:outline-none" @click="expanded = !expanded"><span x-text="expanded ? 'Show less' : 'Show more'"></span>
                  <svg class="w-4 h-4 transform transition-transform duration-300" :class="expanded ? 'rotate-180' : ''" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"></svg>
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
                </button>
                <button class="view-paper-button mt-4 mb-8 inline-block px-6 py-2 text-white bg-green-500 hover:bg-green-600 rounded-lg text-sm" data-paper-id="paper1">View Paper</button>
              </div>
              <div class="w-full md:w-3/4 px-4 lg:px-12" x-data="{ expanded: false }">
                <h2 class="font-heading text-2xl text-white tracking-tighter-xl">Domain randomization for transferring deep neural networks from simulation to the real world</h2>
                <p class="transition-all duration-300" :class="expanded ? 'text-gray-300 tracking-wider' : 'text-gray-300 line-clamp-1'">
                  Abstract—Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic
                  research through improved data availability. This paper ex- plores domain randomization, a simple technique for training
                  models on simulated images that transfer to real images by ran- domizing rendering in the simulator. With enough variability in
                  the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization,
                  which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object
                  detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with
                  non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping
                  in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on
                  simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.
                </p>
                <button class="text-green-400 text-sm font-medium flex items-center space-x-1 hover:underline focus:outline-none" @click="expanded = !expanded"><span x-text="expanded ? 'Show less' : 'Show more'"></span>
                  <svg class="w-4 h-4 transform transition-transform duration-300" :class="expanded ? 'rotate-180' : ''" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"></svg>
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
                </button>
                <button class="view-paper-button mt-4 mb-8 inline-block px-6 py-2 text-white bg-green-500 hover:bg-green-600 rounded-lg text-sm" data-paper-id="paper2">View Paper</button>
              </div>
            </div>
          </div>
          <div class="md:max-w-xl text-center mx-auto mb-10 mt-20">
            <h2 class="font-heading text-7xl text-white tracking-tighter-xl">About us</h2>
          </div>
          <div class="max-w-5xl mx-auto mb-10">
            <div class="flex flex-wrap items-center justify-center mx-auto">
              <div class="w-full md:w-3/4 px-4 lg:px-12">
                <p class="mb-4 text-gray-300 tracking-wider">
                   We’re a team of engineers and roboticists with backgrounds in 
                  perception, control, mechanical design, and manufacturing. 
                  Time and again, we hit the same wall: the AI and reinforcement 
                  learning tools are here, ready to deploy, but the data wasn’t. 
                  Public datasets rarely fit the task, and collecting and 
                  labeling real-world data was slow, expensive, and painful.
                </p>
                <p class="mb-4 text-gray-300 tracking-wider">
                   So we built our own pipeline, one that generates exactly the 
                  data you need, in the context you need it.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <section class="bg-gray-50 overflow-hidden">
        <div class="py-14 bg-body rounded-b-5xl"></div>
        <div class="py-12">
          <div class="container px-4 mx-auto">
            <div class="flex flex-wrap justify-center -m-8 mb-28">
              <div class="w-full md:w-1/2 lg:w-4/12 p-8">
                <div class="flex flex-col md:max-w-md items-center"><img class="mb-7" src="images/logo-dark-transparent.png" alt=""/>
                  <p class="text-gray-400 font-medium">Take your computer vision capabilities to new heights</p>
                </div>
              </div>
              <div class="w-full md:w-1/2 lg:w-2/12 p-8">
                <ul>
                  <li class="mb-2.5"><a class="inline-block text-lg font-medium text-gray-500 hover:text-black transition duration-300" href="index.html">Home</a></li>
                  <li class="mb-2.5"><a class="inline-block text-lg font-medium text-gray-500 hover:text-black transition duration-300" href="about.html">About</a></li>
                  <li class="mb-2.5"><a class="inline-block text-lg font-medium text-gray-500 hover:text-black transition duration-300" href="gallery.html">Gallery</a></li>
                  <li class="mb-2.5"><a class="inline-block text-lg font-medium text-gray-500 hover:text-black transition duration-300" href="documentation.html">Documentation</a></li>
                  <li class="mb-2.5"><a class="inline-block text-lg font-medium text-gray-500 hover:text-black transition duration-300" href="contact.html">Contact</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </section>
    </div>
  </body>
  <div class="hidden fixed inset-0 z-50 flex items-center justify-center p-4" id="pdf-modal">
    <div class="bg-white w-full max-w-5xl h-[80vh] rounded-xl shadow-lg overflow-hidden relative" id="pdf-content" onclick="event.stopPropagation()">
      <button class="absolute top-3 right-4 text-black text-2xl font-bold hover:text-red-500" id="close-pdf-modal">×</button>
      <iframe class="w-full h-full" id="pdf-frame" type="application/pdf"></iframe>
    </div>
  </div>
</html>
<script>
  const paperPaths = {
    paper1: "template-assets/pdf/Synthetic_Image_Data_for_Deep_Learning.pdf",
    paper2: "template-assets/pdf/Domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world.pdf",
    // Add more papers here
  };
  
  const modal = document.getElementById("pdf-modal");
  const frame = document.getElementById("pdf-frame");
  
  // Open modal
  document.querySelectorAll(".view-paper-button").forEach(button => {
    button.addEventListener("click", () => {
      const paperId = button.dataset.paperId;
      const pdfPath = paperPaths[paperId];
  
      if (pdfPath) {
        frame.src = pdfPath;
        modal.classList.remove("hidden");
      }
    });
  });
  
  // Close when clicking the close button
  document.getElementById("close-pdf-modal").addEventListener("click", () => {
    modal.classList.add("hidden");
    frame.src = "";
  });
  
  // Close when clicking the backdrop (anywhere outside the content)
  document.getElementById("pdf-modal").addEventListener("click", () => {
    modal.classList.add("hidden");
    frame.src = "";
  });
</script>