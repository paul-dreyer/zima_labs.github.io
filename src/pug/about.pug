html(lang="en")
    head
        meta(charset="utf-8")
        meta(name="viewport", content="width=device-width, initial-scale=1, shrink-to-fit=no")
        link(rel="preconnect", href="https://fonts.gstatic.com")
        link(href="https://fonts.googleapis.com/css2?family=Playfair+Display&display=swap", rel="stylesheet")
        link(href="https://api.fontshare.com/v2/css?f[]=clash-grotesk@400,300,500&display=swap", rel="stylesheet")
        link(rel="stylesheet", href="css/tailwind/tailwind.min.css")
        link(rel="icon", type="image/png", sizes="32x32", href="fav-icon.png")
        script(src="https://cdn.jsdelivr.net/npm/alpinejs@3.13.3/dist/cdn.min.js", defer="defer")
    
    body(class="antialiased bg-body text-body font-body")
        div(class="")
            include components/_navbar.pug

            section(class="py-20 overflow-hidden")
                div(class="container px-4 mb-20 mx-auto")

                    div(class="md:max-w-xl text-center mx-auto mb-10")
                        h2(class="font-heading text-7xl text-white tracking-tighter-xl") About Synthetic Data

                    div(class="max-w-5xl mx-auto")
                        div(class="flex flex-wrap items-center justify-center mx-auto")
                            div(class="w-full md:w-3/4 px-4 lg:px-12")
                                h2(class="font-heading text-3xl text-green-300 tracking-tighter-xl mb-2") Curriculum Learning
                                p(class="mb-4 text-gray-300 tracking-wider") 
                                    | In robotics, control policies are typically trained on simple 
                                    | tasks first — like balancing — before progressing to more 
                                    | complex challenges such as walking or stair climbing. This 
                                    | approach, known as curriculum learning, helps models learn 
                                    | more effectively. Yet in computer vision, it's rarely applied,
                                    | largely because it's hard to define how “difficult” an image 
                                    | is for a model to interpret.

                                p(class="mb-10 text-gray-300 tracking-wider") 
                                    | Our datasets change that. Every image-label pair includes rich
                                    | metadata that captures scene complexity — like object 
                                    | occlusion, position and  orientation, lighting conditions, etc.
                                    | — making it easy to structure a learning curriculum. Want to 
                                    | begin with clear, unobstructed views and gradually introduce 
                                    | clutter and occlusion? Done. Our synthetic data makes 
                                    | curriculum learning in vision not only possible — but practical.

                    div(class="max-w-5xl mx-auto")
                        div(class="flex flex-wrap items-center justify-center mx-auto")
                            div(class="w-full md:w-3/4 px-4 lg:px-12")
                                h2(class="font-heading text-3xl text-green-300 tracking-tighter-xl mb-2") Domain Randomization
                                p(class="mb-4 text-gray-300 tracking-wider") 
                                    | To build models that generalize to the real world, it's 
                                    | critical to expose them to wide variations during training — 
                                    | a practice known as domain randomization. With synthetic data,
                                    | you have full control: vary lighting, textures, object 
                                    | placement, camera angles, even add fog, dust, or simulated 
                                    | wear.
                                p(class="mb-10 text-gray-300 tracking-wider") 
                                    | Want to train for warehouse conditions with poor lighting and 
                                    | dust buildup? Simulate it. Need your model to handle objects 
                                    | flipped, stacked, or partially occluded? No problem. Our 
                                    | datasets retain all randomization parameters, so you can 
                                    | filter, analyze, or subset based on them before and after training.

                    div(class="max-w-5xl mx-auto")
                        div(class="flex flex-wrap items-center justify-center mx-auto")
                            div(class="w-full md:w-3/4 px-4 lg:px-12")
                                h2(class="font-heading text-3xl text-green-300 tracking-tighter-xl mb-2") Related Papers

                            //- Paper Block
                            div(x-data="{ expanded: false }", class="w-full md:w-3/4 px-4 lg:px-12")
                                h2(class="font-heading text-2xl text-white tracking-tighter-xl")
                                    | Synthetic Image Data for Deep Learning

                                //- Collapsible paragraph
                                p(
                                    :class="expanded ? 'text-gray-300 tracking-wider' : 'text-gray-300 line-clamp-1'",
                                    class="transition-all duration-300"
                                )
                                    | Abstract—Realistic synthetic image data rendered from 3D
                                    | models can be used to augment image sets and train image
                                    | classification semantic segmentation models. In this work, we
                                    | explore how high quality physically-based rendering and domain
                                    | randomization can efficiently create a large synthetic dataset
                                    | based on production 3D CAD models of a real vehicle. We use this
                                    | dataset to quantify the effectiveness of synthetic augmentation
                                    | using U-net and Double-U-net models. We found that, for
                                    | this domain, synthetic images were an effective technique for
                                    | augmenting limited sets of real training data. We observed that
                                    | models trained on purely synthetic images had a very low mean
                                    | prediction IoU on real validation images. We also observed that
                                    | adding even very small amounts of real images to a synthetic
                                    | dataset greatly improved accuracy, and that models trained on
                                    | datasets augmented with synthetic images were more accurate
                                    | than those trained on real images alone.
                                    | Finally, we found that in use cases that benefit from incremen-
                                    | tal training or model specialization, pretraining a base model on
                                    | synthetic images provided a sizeable reduction in the training cost
                                    | of transfer learning, allowing up to 90% of the model training
                                    | to be front-loaded.

                                //- Toggle arrow
                                button(
                                    @click="expanded = !expanded"
                                    class="text-green-400 text-sm font-medium flex items-center space-x-1 hover:underline focus:outline-none"
                                )
                                    span(x-text="expanded ? 'Show less' : 'Show more'")
                                    svg(:class="expanded ? 'rotate-180' : ''", class="w-4 h-4 transform transition-transform duration-300", fill="none", stroke="currentColor", viewBox="0 0 24 24", xmlns="http://www.w3.org/2000/svg")
                                    path(stroke-linecap="round", stroke-linejoin="round", stroke-width="2", d="M19 9l-7 7-7-7")

                                button.view-paper-button(
                                    data-paper-id="paper1",
                                    class="mt-4 mb-8 inline-block px-6 py-2 text-white bg-green-500 hover:bg-green-600 rounded-lg text-sm"
                                ) View Paper

                            
                            //- Paper Block
                            div(x-data="{ expanded: false }", class="w-full md:w-3/4 px-4 lg:px-12")
                                h2(class="font-heading text-2xl text-white tracking-tighter-xl")
                                    | Domain randomization for transferring deep neural networks from simulation to the real world

                                //- Collapsible paragraph
                                p(
                                    :class="expanded ? 'text-gray-300 tracking-wider' : 'text-gray-300 line-clamp-1'",
                                    class="transition-all duration-300"
                                )
                                    | Abstract—Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic
                                    | research through improved data availability. This paper ex- plores domain randomization, a simple technique for training
                                    | models on simulated images that transfer to real images by ran- domizing rendering in the simulator. With enough variability in
                                    | the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization,
                                    | which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object
                                    | detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with
                                    | non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping
                                    | in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on
                                    | simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.

                                //- Toggle arrow
                                button(
                                    @click="expanded = !expanded"
                                    class="text-green-400 text-sm font-medium flex items-center space-x-1 hover:underline focus:outline-none"
                                )
                                    span(x-text="expanded ? 'Show less' : 'Show more'")
                                    svg(:class="expanded ? 'rotate-180' : ''", class="w-4 h-4 transform transition-transform duration-300", fill="none", stroke="currentColor", viewBox="0 0 24 24", xmlns="http://www.w3.org/2000/svg")
                                    path(stroke-linecap="round", stroke-linejoin="round", stroke-width="2", d="M19 9l-7 7-7-7")

                                button.view-paper-button(
                                    data-paper-id="paper2",
                                    class="mt-4 mb-8 inline-block px-6 py-2 text-white bg-green-500 hover:bg-green-600 rounded-lg text-sm"
                                ) View Paper

                        
                    div(class="md:max-w-xl text-center mx-auto mb-10 mt-20")
                        h2(class="font-heading text-7xl text-white tracking-tighter-xl") About us
                    div(class="max-w-5xl mx-auto mb-10")
                        div(class="flex flex-wrap items-center justify-center mx-auto")
                            div(class="w-full md:w-3/4 px-4 lg:px-12")
                                p(class="mb-4 text-gray-300 tracking-wider") 
                                    | We’re a team of engineers and roboticists with backgrounds in 
                                    | perception, control, mechanical design, and manufacturing. 
                                    | Time and again, we hit the same wall: the AI and reinforcement 
                                    | learning tools are here, ready to deploy, but the data wasn’t. 
                                    | Public datasets rarely fit the task, and collecting and 
                                    | labeling real-world data was slow, expensive, and painful.

                                p(class="mb-4 text-gray-300 tracking-wider") 
                                    | So we built our own pipeline, one that generates exactly the 
                                    | data you need, in the context you need it.
            
            //- Page bottom
            include components/_page_bottom.pug

    //- PDF Modal
    div#pdf-modal.hidden.fixed.inset-0.z-50.flex.items-center.justify-center.p-4
        div#pdf-content(
            class="bg-white w-full max-w-5xl h-[80vh] rounded-xl shadow-lg overflow-hidden relative"
            onclick="event.stopPropagation()"
        )
            button#close-pdf-modal(
            class="absolute top-3 right-4 text-black text-2xl font-bold hover:text-red-500"
            ) ×
            iframe#pdf-frame(
            class="w-full h-full"
            type="application/pdf"
            )



script.
  const paperPaths = {
    paper1: "template-assets/pdf/Synthetic_Image_Data_for_Deep_Learning.pdf",
    paper2: "template-assets/pdf/Domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world.pdf",
    // Add more papers here
  };

  const modal = document.getElementById("pdf-modal");
  const frame = document.getElementById("pdf-frame");

  // Open modal
  document.querySelectorAll(".view-paper-button").forEach(button => {
    button.addEventListener("click", () => {
      const paperId = button.dataset.paperId;
      const pdfPath = paperPaths[paperId];

      if (pdfPath) {
        frame.src = pdfPath;
        modal.classList.remove("hidden");
      }
    });
  });

  // Close when clicking the close button
  document.getElementById("close-pdf-modal").addEventListener("click", () => {
    modal.classList.add("hidden");
    frame.src = "";
  });

  // Close when clicking the backdrop (anywhere outside the content)
  document.getElementById("pdf-modal").addEventListener("click", () => {
    modal.classList.add("hidden");
    frame.src = "";
  });